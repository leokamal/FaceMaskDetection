{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2286,"status":"ok","timestamp":1672997551096,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"FWkQR9WMHx6f"},"outputs":[],"source":["# import the necessary packages\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.models import load_model\n","#from imutils.video import VideoStream\n","import numpy as np\n","#import imutils\n","import time\n","import cv2\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CEnBUATcarqC","executionInfo":{"status":"ok","timestamp":1672997574012,"user_tz":-60,"elapsed":21285,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"}},"outputId":"370f10b5-f457-4020-88a2-024198114f07"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":290,"status":"ok","timestamp":1672997576468,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"2sqUzgfkHx6k"},"outputs":[],"source":["def detect_and_predict_mask(frame, faceNet, maskNet):\n","    # grab the dimensions of the frame and then construct a blob\n","    # from it\n","    (h, w) = frame.shape[:2]\n","    blob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n","        (104.0, 177.0, 123.0))\n","\n","    # pass the blob through the network and obtain the face detections\n","    faceNet.setInput(blob)\n","    detections = faceNet.forward()\n","    print(detections.shape)\n","\n","    # initialize our list of faces, their corresponding locations,\n","    # and the list of predictions from our face mask network\n","    faces = []\n","    locs = []\n","    preds = []\n","\n","    # loop over the detections\n","    for i in range(0, detections.shape[2]):\n","        # extract the confidence (i.e., probability) associated with\n","        # the detection\n","        confidence = detections[0, 0, i, 2]\n","\n","        # filter out weak detections by ensuring the confidence is\n","        # greater than the minimum confidence\n","        if confidence > 0.5:\n","            # compute the (x, y)-coordinates of the bounding box for\n","            # the object\n","            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n","            (startX, startY, endX, endY) = box.astype(\"int\")\n","\n","            # ensure the bounding boxes fall within the dimensions of\n","            # the frame\n","            (startX, startY) = (max(0, startX), max(0, startY))\n","            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n","\n","            # extract the face ROI, convert it from BGR to RGB channel\n","            # ordering, resize it to 224x224, and preprocess it\n","            face = frame[startY:endY, startX:endX]\n","            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n","            face = cv2.resize(face, (224, 224))\n","            face = img_to_array(face)\n","            face = preprocess_input(face)\n","\n","            # add the face and bounding boxes to their respective\n","            # lists\n","            faces.append(face)\n","            locs.append((startX, startY, endX, endY))\n","\n","    # only make a predictions if at least one face was detected\n","    if len(faces) > 0:\n","        # for faster inference we'll make batch predictions on *all*\n","        # faces at the same time rather than one-by-one predictions\n","        # in the above `for` loop\n","        faces = np.array(faces, dtype=\"float32\")\n","        preds = maskNet.predict(faces, batch_size=32)\n","\n","    # return a 2-tuple of the face locations and their corresponding\n","    # locations\n","    return (locs, preds)"]},{"cell_type":"markdown","source":["#load our serialized face detector model from Google drive"],"metadata":{"id":"GVT_BefjBTHR"}},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1919,"status":"ok","timestamp":1672997581004,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"O_cxr4q5Hx6m"},"outputs":[],"source":["# load our serialized face detector model from Google drive\n","#deploy.prototxt contains the details of the model architecture for the face detection model\n","prototxtPath = r\"/content/drive/MyDrive/AIC/TP/Project AI Face Mask Detection/face_detector/deploy.prototxt\"\n","#res10_300x300_ssd_iter_140000.caffemodel has the pre-trained model weights for face detection\n","weightsPath = r\"/content/drive/MyDrive/AIC/TP/Project AI Face Mask Detection/face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n","faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5396,"status":"ok","timestamp":1672997588727,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"Q2cHZnmHHx6m"},"outputs":[],"source":["# load the face mask detector model from disk\n","maskNet = load_model(\"/content/drive/MyDrive/AIC/TP/Project AI Face Mask Detection/Model/mask_detector.model\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9eC9YysPHx6n","executionInfo":{"status":"ok","timestamp":1672997590353,"user_tz":-60,"elapsed":311,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"}}},"outputs":[],"source":["# initialize the video stream\n","#print(\"[INFO] starting video stream...\")\n","#vs = VideoStream(src='/content/drive/MyDrive/AIC/TP/Project AI Face Mask Detection').start()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1672997593649,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"o6_WxOR7hdNQ"},"outputs":[],"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1672997597379,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"qda-s2SDh3nW"},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"markdown","source":["Create our live video stream using our webcam as input"],"metadata":{"id":"iZhNZQMIDyCK"}},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1672997617623,"user":{"displayName":"EL MAGOURI Kamal","userId":"18333068682442269445"},"user_tz":-60},"id":"sMWNYcPUgSUf"},"outputs":[],"source":["\n","# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","while True:\n","    # grab the frame from the threaded video stream and resize it\n","    # to have a maximum width of 400 pixels\n","    ##frame = vs.read()\n","    ##frame = imutils.resize(frame, width=400)\n","\n","    \n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    frame = js_to_image(js_reply[\"img\"])\n","\n","    # detect faces in the frame and determine if they are wearing a\n","    # face mask or not\n","    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n","\n","    # loop over the detected face locations and their corresponding\n","    # locations\n","    for (box, pred) in zip(locs, preds):\n","        # unpack the bounding box and predictions\n","        (startX, startY, endX, endY) = box\n","        (mask, withoutMask) = pred\n","\n","        # determine the class label and color we'll use to draw\n","        # the bounding box and text\n","        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n","        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n","\n","        # include the probability in the label\n","        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n","\n","        # display the label and bounding box rectangle on the output\n","        # frame\n","        cv2.putText(frame, label, (startX, startY - 10),\n","            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n","        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n","\n","    # show the output frame\n","    cv2_imshow(frame)\n","    key = cv2.waitKey(1) & 0xFF\n","\n","    # if the `q` key was pressed, break from the loop\n","    if key == ord(\"q\"):\n","        break"],"metadata":{"id":"_2wvWsFKa6d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9M0MtfpgHx6o"},"outputs":[],"source":["# do a bit of cleanup\n","cv2.destroyAllWindows()\n","vs.stop()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":0}